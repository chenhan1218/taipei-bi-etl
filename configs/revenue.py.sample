"""Configurations for data source, data schema, and data destinations."""
# flake8: noqa
import numpy as np

"""CSV data source config samples."""

SOURCES = {
    """Constant data source config sample.

    values: constant data in a list of dictionaries.
    """
    "const_source": {
        "type": "const",
        "values": [{"utc_datetime": "2019-01-01 00:00:00", "tz": "+0800"}],
    },
    """CSV API data source config sample.

    type (required): the type of the data source
    url (required): the url of the API including parameters like start date, end date, api key, etc...
    api_key (required): the API key for the request, if no API key required, leave it blank
    date_format (required): the date format of the start date, end date parameters
    file_format (required): the response file format
    header: provide header names if not exists in raw data
    cache_file: whether to save/load cached files to avoid hitting API too many times.
    file_format: can be csv, json
    date_format: date format of the start/end date
    """
    "api_csv_source": {
        "type": "api",
        "url": "https://api.test.com/resource.csv?api_key={api_key}&date_range={start_date}%2C{end_date}",
        "api_key": "xxxxxxx",
        "header": ["platform", "category", "country_code", "country", "cpi", "month"],
        "cache_file": True,
        "date_format": "%Y-%m-%d",
        "file_format": "csv",
    },
    """API data source config sample.

    type (required): the type of the data source
    url (required): the url of the API including parameters like start date, end date, api key, etc...
    api_key (required): the API key for the request, if no API key required, leave it blank
    date_format (required): the date format of the start date, end date parameters
    file_format (required): the response file format
    cache_file: whether to save/load cached files to avoid hitting API too many times.
    iterator: specify here if there are multiple values of a parameter to iterate from the API
    """
    "api_iterator": {
        "type": "api",
        "url": "https://api.test.com/resource.csv?id={iterator}&date_range={start_date}%2C{end_date}",
        "api_key": "",
        "cache_file": True,
        "date_format": "%Y-%m-%d",
        "file_format": "json",
        "iterator": range(1, 18),
    },
    """API data source config sample.

    type (required): the type of the data source
    url (required): the url of the API including parameters like start date, end date, api key, etc...
    api_key (required): the API key for the request, if no API key required, leave it blank
    date_format (required): the date format of the start date, end date parameters
    file_format (required): the response file format
    cache_file: whether to save/load cached files to avoid hitting API too many times.
    load: default false, whether to load data into destination, not needed for supplementary data sets.
    force_load_cache: default false, which means only load data which just returned from source,
        true means it will load local cached files to destinations when cache hit.
    """
    "api_single_call": {
        "type": "api",
        "url": "https://api.test.com/resource.csv?api_key={api_key}",
        "api_key": "gVvrRBTUGOBpGJmTDAL4HgHf1KqPvyAbbs1d3yny",
        "cache_file": True,
        "date_format": "%Y-%m-%d",
        "file_format": "csv",
        "load": True,
        "force_load_cache": False,
    },
    """API json data source config sample.

    type (required): the type of the data source
    url (required): the url of the API including parameters like start date, end date, api key, etc...
    api_key (required): the API key for the request, if no API key required, leave it blank
    date_format (required): the date format of the start date, end date parameters
    file_format (required): the response file format
    request_interval: interval in seconds between API requests, to avoid being blocked by some APIs
    json_path: the json path to extract data from
    json_path_page_count: the json path of the page count data
    page_size: page size per request
    country_code: for timezone conversion (assuming same timezone for single source)
    datetime_fields: specify datatime fields to convert datetime strings into datetime format.
    """
    "api_paged_json": {
        "type": "api",
        "url": "https://api.test.com/resource.csv?api_key={api_key}&start_date={start_date}&end_date={end_date}",
        "api_key": "j28fh2h38f0hh238f038gf",
        "load": True,
        "request_interval": 1,
        "cache_file": True,
        "force_load_cache": False,
        "date_format": "%Y-%m-%d",
        "file_format": "json",
        "json_path": "response.data.data",
        "json_path_page_count": "response.data.pageCount",
        "page_size": 100,
        "country_code": "ID",
        "datetime_fields": ["Stat.date", "Stat.datetime", "Stat.session_datetime"],
    },
    """GCS data source config sample.

    type (required): the type of the data source
    bucket (required): the GCS bucket
    prefix (required): the prefix of the data source
    path (required): the path (after prefix) of the data source
    filename (required): the file name (after path) of the data source, should support wildcard *
    file_format (required): the response file format
    """
    "gcs_single_file": {
        "type": "gcs",
        "bucket": "moz-fx-data",
        "prefix": "taipei/",
        "path": "staging-rps-google_search_rps/",
        "filename": "2018-01-01.csv",
        "file_format": "csv",
    },
    """BigQuery data source config sample.

    project: the BigQuery project name of the table
    dataset: the BigQuery dataset name of the table
    table: the BigQuery table name
    udf: a list of user-defined functions, use the sql filenames w/o extension in the `udf` folder
    udf_js: a list of user-defined js functions, use the sql filenames w/o extension in the `udf_js` folder
    sql: the sql query filename in the `sql` folder
    """
    "bq": {
        "type": "bq",
        "project": "moz-fx-prod",
        "dataset": "telemetry",
        "table": "focus_event",
        "udf_js": ["json_extract_events"],
        "query": "revenue_search_events",
        "load": True,
        "date_format": "%Y-%m-%d",
    },
}

"""Schema (to load to destination) config sample.

Schema format: List[Tuple[str: column name, np.generic: column type]]
"""
SCHEMA = [
    ("source", np.dtype(object).type),
    ("country", np.dtype(object).type),
    ("os", np.dtype(object).type),
    ("utc_datetime", np.datetime64),
    ("tz", np.dtype(object).type),
    ("currency", np.dtype(object).type),
    ("sales_amount", np.dtype(float).type),
    ("payout", np.dtype(float).type),
    ("fx_defined1", np.dtype(object).type),
    ("fx_defined2", np.dtype(object).type),
    ("fx_defined3", np.dtype(object).type),
    ("fx_defined4", np.dtype(object).type),
    ("fx_defined5", np.dtype(object).type),
    ("conversion_status", np.dtype(object).type),
]

"""Data destination config samples."""
DESTINATIONS = {
    """Destination config sample for GCS.

    bucket (required): the GCS bucket
    prefix (required): the prefix of the data source
    """
    "gcs": {
        "bucket": "moz-fx-data",
        "prefix": "taipei/",
    },
    """Destination config sample for file system.

    prefix (required): the local path prefix of the data source
    file_format (required): the destination file format
    date_fields: specify data field to convert datetime strings into datetime format, assuming only one date field.
    """
    "fs": {
        "prefix": "./data/",
        "file_format": "jsonl",
        "date_field": "utc_datetime",
    }
}
